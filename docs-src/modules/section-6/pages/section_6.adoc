:imagesdir: ../images

== SECTION 6. BASIC SORTING ALGORITHMS

=== 6.1. General Characteristics of Sorting Algorithms

Sorting is one of the most common operations in data processing — not only in computer science, but in many areas of human activity. In computing, sorting algorithms are used everywhere: in databases, search engines, operating systems, and beyond.

At its core, the goal of sorting is simple: given a list of records (each with a key value), we want to reorder the list so that the keys are arranged in increasing or decreasing order. This makes it easier to work with the data — especially when we need to search or analyze it later.

Over time, many sorting algorithms have been developed. Some are easy to implement and work well for small datasets, but become inefficient as the dataset grows. Others are designed for large collections and offer better performance, though they may be more complex to understand or implement.

To help understand how these algorithms work, especially the more complex ones, we’ll use visual explanations. In this book, we include DRAKON-diagrams and illustrations to make the logic clear and accessible.

Since most sorting tasks in programming involve arrays or similar structures, this section focuses on sorting slices (contiguous segments of arrays).

=== 6.2. Bubble Sort

Bubble Sort is one of the simplest sorting algorithms, especially suitable for small datasets. The algorithm gets its name from the way larger values "bubble up" to the end of the array with each pass. The DRAKON-diagram of the algorithm is shown in Figure 6.1. The implementation consists of two modules: `main()` and `bubbleSort(arr)`.


image::Fig6_1_bubbleSort.svg[width=500, height=300]

[.text-center]
Figure 6.1. DRAKON-diagram of the Bubble Sort algorithm

The algorithm uses two nested loops. For each index `i`, it iterates over the remaining elements with index `j`. At each step, the values `ar[i]` and `ar[j]` are compared. If `ar[j]` is smaller than `ar[i]`, the two elements are swapped. This process pushes the largest unsorted value toward the end of the array during each outer loop iteration.

The following table shows the results of sorting a series of values for each external cycle parameter (i).

[.text-center]

[cols="^1,^1,^1,^1,^1,^1,^1,^1,^1,^1", options="header"]
|===
|j=0 |j=1 |j=2 |j=3 |j=4 |j=5 |j=6 |j=7 |j=8| j=9 
| 90 | 11 | 81 | 21 | 71 | 31 | 61 | 41 | 51 | 10  
| 11 | 90 | 81 | 21 | 71 | 31 | 61 | 41 | 51 | 10 
| 11 | 81 | 90 | 21 | 71 | 31 | 61 | 41 | 51 | 10 
| 11 | 21 | 81 | 90 | 71 | 31 | 61 | 41 | 51 | 10 
| 11 | 21 | 71 | 81 | 90 | 31 | 61 | 41 | 51 | 10 
| 11 | 21 | 31 | 71 | 81 | 90 | 61 | 41 | 51 | 10 
| 11 | 21 | 31 | 61 | 71 | 81 | 90 | 41 | 51 | 10  
| 11 | 21 | 31 | 51 | 61 | 71 | 81 | 90 | 51 | 10 
| 11 | 21 | 31 | 41 | 51 | 61 | 71 | 81 | 90 | 10 
| 10 | 11 | 21 | 31 | 41 | 51 | 61 | 71 | 81 | 90 
|===

The time complexity of Bubble Sort is quite high — O(n²). This is due to the large number of comparisons (`ar[i] < ar[j]`) and possible swaps. However, the space complexity is O(1), since no additional memory is needed beyond the original array.

The algorithm is stable, meaning that equal elements retain their original order after sorting.

Because of its simplicity, Bubble Sort is still used in some applications — for example, in computer graphics. There, it is helpful for correcting small errors in nearly sorted arrays, often completing in linear time (about 2n steps). However, it is highly inefficient for large datasets.

=== 6.3. Selection Sort

The Selection Sort algorithm is based on comparison operations. It works by dividing the dataset into two parts: the sorted portion on the left and the unsorted portion on the right. The DRAKON-diagram of this algorithm is shown in Figure 6.2.

++++
<object type="image/svg+xml" data="images/Fig6_2_selectSort" width="100%"></object>
++++

[.text-center]
Figure 6.2. DRAKON-diagram of the Selection Sort algorithm

The idea is simple: in each iteration, the algorithm finds the smallest value in the unsorted part and swaps it with the first unsorted element. Initially, the sorted part is empty, and the unsorted part is the entire array.

For example, given the set `[90, 12, 83, 24, 75, 38, 62, 41, 59, 10]`, the algorithm scans the array and finds the smallest element — `10`. It then swaps `10` with the element at the first position (`90`). The sorted part now contains `10`, and the process repeats with the remaining elements. Table below illustrates this process of selection and replacement.

[cols="10"]
|===
| *90*  | *12* | 83   | 24   | 75   | 38   | 62   | 41   | 59   | 10  
| [.line-through]#10# | 12   | 83   | 24   | 75   | 38   | 62   | 41   | 59   | *90*  
| [.line-through]#10# | *12* | 83   | 24   | 75   | 38   | 62   | 41   | 59   | [.line-through]#90#  
| [.line-through]#10# | [.line-through]#12# | 83   | *24* | 75   | 38   | 62   | 41   | 59   | [.line-through]#90#  
| [.line-through]#10# | [.line-through]#12# | [.line-through]#24# | 83   | 75   | *38* | 62   | 41   | 59   | [.line-through]#90#  
| [.line-through]#10# | [.line-through]#12# | [.line-through]#24# | [.line-through]#38# | 75   | 83   | 62   | *41* | 59   | [.line-through]#90#  
| [.line-through]#10# | [.line-through]#12# | [.line-through]#24# | [.line-through]#38# | [.line-through]#41# | 83   | 62   | 75   | *59* | [.line-through]#90#  
| [.line-through]#10# | [.line-through]#12# | [.line-through]#24# | [.line-through]#38# | [.line-through]#41# | [.line-through]#59# | 62   | *75* | 83   | [.line-through]#90#  
| [.line-through]#10# | [.line-through]#12# | [.line-through]#24# | [.line-through]#38# | [.line-through]#41# | [.line-through]#59# | [.line-through]#62# | 75   | *83* | [.line-through]#90#  
| [.line-through]#10# | [.line-through]#12# | [.line-through]#24# | [.line-through]#38# | [.line-through]#41# | [.line-through]#59# | [.line-through]#62# | [.line-through]#75# | [.line-through]#83# | [.line-through]#90#  
|===  

* Note to the table. Crossed-out items are already sorted, selected items are minimal in the current pass.

The time and space complexity of the Selection Sort algorithm are summarized in the table below:

[cols="2*", width="40%", options="header"]
|===
| Case               | Time Complexity
| Worst case         | O(n^2^)
| Average case       | O(n^2^)
| Best case          | O(n^2^)
| *Space complexity* | O(1)
|===

The time complexity is the same in all cases because the algorithm uses two nested loops:  
– The outer loop runs `n` times (once per element);  
– The inner loop runs `(n - 1)` times per outer iteration.  

This results in a total of `n * (n - 1)` comparisons, which gives the time complexity of O(n²) regardless of the input order.

=== 6.4. Insertion Sort

Insertion Sort works by repeatedly taking an item from the unsorted part of the array and inserting it into its correct position in the sorted part. This process continues until all items are sorted.

This algorithm is similar to how people naturally sort playing cards or paper documents: taking one item at a time and inserting it into the correct place. The DRAKON-diagram for the Insertion Sort algorithm is shown in Figure 6.3.

++++
<object type="image/svg+xml" data="images/Fig6_3_insertSort" width="100%"></object>
++++

[.text-center]
Figure 6.3. DRAKON-diagram of the Insert Sort algorithm

In this algorithm, larger values are pushed to the right. The algorithm compares adjacent elements and swaps them if needed. This process is shown in table below, where underscores mark the positions where swaps occurred.

[.text-center]
Input array

[cols="10"]
|===
|90 |11 |81 |21 |71 |31 |61 |41 |51 |10
|===

[.text-center]
I = 1

[cols="10"]
|===
|11 |90 |81 |21 |71 |31 |61 |41 |51 |10
|===

[.text-center]
I = 2

[cols="10]
|===
|11 |81 |90 |21 |71 |31 |61 |41 |51 |10
|===

[.text-center]
I = 3

[cols="10"]
|===
|11 |81 |21 |90 |71 |31 |61 |41 |51 |10
|11 |21 |81 |90 |71 |31 |61 |41 |51 |10
|===

[.text-center]
I = 4

[cols="10"]
|===
|11 |21 |81 |71 |90 |31 |61 |41 |51 |10
|11 |21 |71 |81 |90 |31 |61 |41 |51 |10
|===

[.text-center]
I = 5

[cols="10"]
|===
|11 |21 |71 |81 |31 |90 |61 |41 |51 |10
|11 |21 |71 |31 |81 |90 |61 |41 |51 |10
|11 |21 |31 |71 |81 |90 |61 |41 |51 |10
|===

[.text-center]
I = 6

[cols="10"]
|===
|11 |21 |31 |71 |81 |61 |90 |41 |51 |10
|11 |21 |31 |71 |61 |81 |90 |41 |51 |10
|11 |21 |31 |61 |71 |81 |90 |41 |51 |10
|===

[.text-center]
I = 7

[cols="10"]
|===
|11 |21 |31 |61 |71 |81 |41 |90 |51 |10
|11 |21 |31 |61 |71 |81 |41 |51 |90 |10
|11 |21 |31 |61 |41 |71 |81 |90 |51 |10
|11 |21 |31 |41 |61 |71 |81 |90 |51 |10
|===

[.text-center]
I = 8

[cols="10"]
|===
|11 |21 |31 |41 |61 |71 |81 |51 |90 |10
|11 |21 |31 |41 |61 |71 |51 |81 |90 |10
|11 |21 |31 |41 |61 |51 |71 |81 |90 |10
|11 |21 |31 |41 |51 |61 |71 |81 |90 |10
|===

[.text-center]
I = 9

[cols="10"]
|===
|11 |21 |31 |41 |51 |61 |71 |81 |10 |90
|11 |21 |31 |41 |61 |51 |71 |10 |81 |90
|11 |21 |31 |41 |51 |61 |71 |10 |81 |90
|11 |21 |31 |41 |51 |61 |10 |71 |81 |90
|11 |21 |31 |41 |51 |10 |61 |71 |81 |90
|11 |21 |31 |41 |10 |51 |61 |71 |81 |90
|11 |21 |31 |10 |41 |51 |61 |71 |81 |90
|11 |21 |10 |31 |41 |51 |61 |71 |81 |90
|11 |10 |21 |31 |41 |51 |61 |71 |81 |90
|10 |11 |21 |31 |41 |51 |61 |71 |81 |90
|===

The time complexity of Insertion Sort depends on how the elements are arranged in the input array:

*Worst case:* when the array is sorted in reverse order.  
Each element must be compared with all previous elements and moved to the beginning of the array.  
The number of operations is:

[.text-center]
n(n - 1) / 2 → O(n²)

*Average case:* when the array is partially sorted.  
Each element is compared, on average, with half of the previous elements:

[.text-center]
(n²) / 4 → O(n²)

*Best case:* when the array is already sorted.  
Each element is compared with only the one before it:

[.text-center]
(n - 1) → O(n)

The complexity summary is given in the table below:

[cols="2*", width="40%", options="header"]
|===
  | Time complexity      |                  
  |    Worst case        |  O(n^2^)    
  |   Average case       |  O(n^2^)    
  |      Best case       |  O(n^2^)
  |  *Space complexity*  |  О(1)   
|===

=== 6.5. Quick Sort

Quick Sort is a highly efficient sorting algorithm based on the divide-and-conquer strategy. Its general scheme includes the following steps:

[arabic]
. Select a pivot (reference) element from the array slice.
. Partition the array so that all elements smaller than the pivot are placed before it, and all greater or equal — after it.
. Recursively apply this procedure to the left and right partitions.
. As a result, the array becomes fully sorted.

The DRAKON-diagram of the Quick Sort algorithm is shown in Figure 6.4.

++++
<object type="image/svg+xml" data="images/Fig6_4_quicktSort" width="100%"></object>
++++

[.text-center]
Figure 6.4. DRAKON-diagram of the Quick Sort algorithm

Let’s look at an example in detail. Suppose we are sorting the array `[78, 11, 81, 21, 71, 31, 61, 41, 51, 24]`, and we choose `31` as the pivot.

|===
| 78 | 11 | 81 | 21 | 71 | **31** | 61 | 41 | 51 | 24
|===

We create three temporary arrays:  
– **left[]** for elements less than the pivot,  
– **equal[]** for elements equal to the pivot,  
– **right[]** for elements greater than the pivot.

During execution, if an element from the left side is greater than the pivot, and one from the right side is smaller, the two elements are swapped. In our example, `78 > 31` and `24 < 31`, so they are exchanged. This process continues until the subarrays is partitioned correctly. Then, Quick Sort is applied recursively to the left and right partitions. 

To better understand how recursive sorting builds the final result, the table below shows how the `quickSort` function combines the `left`, `equal`, and `right` parts at each step:

[cols="4,3,1,3,5", options="header"]
|===
| Step | Left | Pivot | Right | Result = left + equal + right

| [78, 11, 81, 21, 71, *31*, 61, 41, 51, 24] 
| 11, 21, 24 
| *31* 
| 78, 81, 71, 61, 41, 51 
| –

| [11, 21, 24] 
| 11 
| *21* 
| 24 
| 11, 21, 24

| [78, 81, 71, 61, *41*, 51] 
| 41, 51 
| *61* 
| 78, 81, 71 
| 11, 21, 24

| [41, 51] 
| 41 
| *51* 
| 
| 11, 21, 24, 41, 51

| [78, 81, 71] 
| 78, 71 
| *81* 
| 
| 11, 21, 24, 41, 51, 61

| [78, 71] 
| 
| *71* 
| 78 
| 11, 21, 24, 41, 51, 61, 71, 78

| [81] 
| 
| *81* 
| 
| 11, 21, 24, 41, 51, 61, 71, 78, 81
|===


The time complexity of Quick Sort depends on how well the pivot is chosen:

- **Best case**: the pivot is the median → O(n log n)
- **Average case**: still O(n log n), due to recursive halving
- **Worst case**: when the pivot is the smallest or largest item, resulting in unbalanced partitions → O(n²)

Quick Sort uses recursion. In each step, it partitions the array and performs O(n) work. This leads to the recurrence relation:  
`T(n) = 2T(n/2) + O(n)`  
which solves to `O(n log n)`.

The complexity summary is shown below. Quick Sort is **not stable** by default.

[cols="2*", width="40%", options="header"]
|===
| Time complexity        |                   
| Worst case             | O(n^2^)     
| Average case           | O(n log n)   
| Best case              | O(n log n)
| *Space complexity*     | 
| Worst case             | O(n)
| Average case           | O(log n) 
|===

=== 6.6. Merge Sort

Merge Sort is a classic divide-and-conquer algorithm. It works by recursively splitting the array into smaller parts until each part contains only two elements, which are then compared and sorted. After the splitting phase, the algorithm merges the sorted parts back together.

During merging, one element from each fragment is selected and compared. The smaller (or larger, depending on the desired order) element is added to the result array. The remaining element is compared again with the next element from the other fragment. This process continues until all elements are merged. Figure 6.5 illustrates this process:

++++
<div style="display: flex; justify-content: center; gap: 2rem; flex-wrap: wrap;">
  <object type="image/svg+xml" data="images/Fig6_5_merge.svg" width="48%"></object>
  <object type="image/svg+xml" data="images/Fig6_5_mergeSort.svg" width="48%"></object>
</div>
++++

[.text-center]
Figure 6.5. DRAKON-diagrams of Merge Sort algorithm: a) `mergeSort()` function  b) `merge()` function

Visual representation of Merge Sort (splitting phase) is shown on Figure 6.6.
[listing]
....
            [11, 32, 23, 85, 74, 90, 62, 48, 53, 10]
                      /                           \
       [11, 32, 23, 85, 74]                [90, 62, 48, 53, 10]
           /         \                          /         \
      [11, 32]     [23, 85, 74]            [90, 62]     [48, 53, 10]
       /   \         /     \               /    \         /     \
   [11]   [32]     [23]   [85, 74]       [90]  [62]     [48]  [53, 10]
                             / \                             / \
                          [85] [74]                      [53] [10]
....
==> Final merge: [11, 23, 32, 74, 85] + [10, 48, 53, 62, 90]  
=> [10, 11, 23, 32, 48, 53, 62, 74, 85, 90]

[.text-center]
Figure 6.6.Visual representation of Merge Sort

Merge Sort has a time complexity of O(n log n), where `n` is the number of elements in the array. The algorithm splits the array into two halves until only single elements remain. The number of split operations is O(log n), and each merge operation compares `n` elements, which gives the overall complexity of O(nlog n).

The summary of time and space complexity is shown below:

[cols="2*", width="40%", options="header"]
|===
| Time complexity        |                   
| Worst case             | O(n log n)     
| Average case           | O(n log n)   
| Best case              | O(n log n)
| *Space complexity*     |                   
| Worst case             | O(n)
| Average case           | O(n)
| Best case              | O(n)
|===

Merge Sort is a **stable** sorting algorithm, which means it preserves the relative order of equal elements in the input array. This stability is achieved because, during the merge phase, if two elements are equal, the one from the left subarray is placed before the one from the right subarray.

This consistent behavior ensures that equal elements remain in the same order as they appeared in the original input.

=== 6.7. Shell Sort

Shell Sort is a variation of Insertion Sort. It begins by comparing and sorting elements that are separated by a certain distance _d_. Then, the process is repeated for smaller values of _d_ until _d = 1_, which is equivalent to a standard insertion sort.

The DRAKON-diagram of the Shell Sort algorithm is shown in Figure 6.7.

++++
<object type="image/svg+xml" data="images/Fig6_7_shelltSort" width="100%"></object>
++++

[.text-center]
Figure 6.7. DRAKON-diagram of the Shell Sort algorithm

Shell Sort uses a three-step process common to many sorting algorithms:  
– segmenting the array,  
– sorting within the segments,  
– merging the results into a sorted array.

The array is partitioned in such a way that each item in a segment is a fixed number of positions apart from the others. This introduces a challenge: choosing the best distance _d_. A simple example of such a sequence is:  
_d = n / 2_, then _d = d / 2_, and so on, until _d = 1_.

The table illustrates how elements are swapped when a smaller value is found at position _arr[j]_ compared to _arr[j - inc]_.

[.text-center]
*inc = 5*

[cols="^1,^1,^1,^1,^1,^1,^1,^1,^1,^1,^1"]
|===
| j | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9
|   | **90** | -80 | 70 | -60 | 50 | **-40** | 30 | -20 | 10 | 0
|   | -40 | -80 | **-20** | -60 | 50 | 90 | 30 | **70** | 10 | 0
|   | -40 | -80 | -20 | -60 | **0** | 90 | 30 | 70 | 10 | **50**
|===

[.text-center]
*inc = 2*

[cols="^1,^1,^1,^1,^1,^1,^1,^1,^1,^1,^1"]
|===
| j | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9
|   | -40 | -80 | -20 | -60 | 0 | **70** | 30 | **90** | 10 | 50
|   | -40 | -80 | -20 | -60 | 0 | 70 | **10** | 90 | **30** | 50
|   | -40 | -80 | -20 | -60 | 0 | 70 | 10 | **90** | 30 | **50**
|   | -40 | -80 | -20 | -60 | 0 | **50** | 10 | **70** | 30 | 90
|===

[.text-center]
*inc = 1*

[cols="^1,^1,^1,^1,^1,^1,^1,^1,^1,^1,^1"]
|===
| j | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9
|   | **-40** | **-80** | -20 | -60 | 0 | 50 | 10 | 70 | 30 | 90
|   | -80 | **-40** | **-60** | -20 | 0 | 50 | 10 | 70 | 30 | 90
|   | -80 | -60 | -40 | -20 | 0 | **50** | **10** | 70 | 30 | 90
|   | -80 | -40 | -60 | -20 | 0 | 10 | 50 | **70** | **30** | 90
|   | -80 | -40 | -60 | -20 | 0 | 10 | **50** | **30** | 70 | 90
|   | -80 | -40 | -60 | -20 | 0 | 10 | 30 | **50** | **70** | 90
|===


The average time complexity of Shell Sort is estimated as _O(n * log² n)_, though this depends on multiple factors.

First, Shell Sort is essentially a generalization of Insertion Sort that uses gap-based comparisons. Second, the performance depends on the choice of gap sequence — that is, the values of _d_ used during sorting. Different sequences can yield significantly different results.

Because Shell Sort works by refining the array through a series of passes with decreasing gaps, its time complexity is sensitive to these gap values. The optimal gap sequence is still a subject of research, and no single sequence is universally optimal.

As a result, although the average-case complexity is commonly cited as _O(n * log² n)_, the exact performance may vary depending on the input and the sequence used.

Shell Sort is **not stable** in its original form, but it can be modified to preserve stability if needed.

=== 6.8. Heapsort

The pyramid sorting algorithm can be seen as an improved version of the
choice sorting algorithm (Select Sort): it divides the input data into
sorted and unreported areas, and then successively reduces the
unreported area, removing the largest item and moving it to the sorted
area. An improvement is that the binary pile is used to find the highest
value, not the linear search algorithm. This algorithm is executed using
the notion of heap, which is a complete binary tree ([.mark]#see
sub-section 1.3#.). All nodes of a heap are either larger than its child
items or smaller than its child items. A heap binary tree can be of two
types: a minimum heap (MinHeap), in which the parent node is always
smaller than the child nodes, and a maximum heap (MaxHeap), in which the
parent node is always greater than or equal to the child nodes (Figure
6.8).

++++
<object type="image/svg+xml" data="images/Fig6_8_Tree" width="100%"></object>
++++

[.text-center]
Figure 6.8. Binary tree examples (a - MinHeap; b - MaxHeap)

The tree node sequence, starting with the root node, is performed by the
formula:

[.text-center]
i_n = \frac{array\ size}{2} - 1


First, the algorithm swaps the nodes (20) and (41), then the nodes (5)
and (52), then we present this process in the table:

[cols=",,,,,,",options="header",]
|===
|23 |5 |20 |52 |11 |41 |14
|23 |5 |*41* |52 |11 |*20* |14
|23 |*52* |41 |*5* |11 |20 |14
|*52* |*23* |41 |5 |11 |20 |14
|*14* |23 |41 |5 |11 |20 |*52*
|*41* |23 |14 |5 |11 |20 |52
|*20* |23 |14 |5 |11 |*41* |52
|*23* |*20* |14 |5 |11 |41 |52
|*11* |20 |14 |5 |*23* |41 |52
|*20* |*11* |14 |5 |23 |41 |52
|*5* |11 |14 |*20* |23 |41 |52
|*11* |*5* |14 |20 |23 |41 |52
|*5* |*11* |14 |20 |23 |41 |52
|===

The heap sorting algorithm uses three functions: _heap_Sort_, which
performs node overwriting, _heapify_, which compares adjacent nodes, and
_swap_, which swap two nodes. The sequence of the nodes in the heap is
shown in Figure 6.9:

[.text-center]
Figure 6.9. Sequence of node movement in heap

DRAKON-diagram of heap algorithm is presented in Figure 6.10:

++++
<div style="display: flex; justify-content: center; gap: 2rem; flex-wrap: wrap;">
  <object type="image/svg+xml" data="images/Fig6_10a_heapSort.svg" width="48%"></object>
  <object type="image/svg+xml" data="images/Fig6_5b_heapify.svg" width="48%"></object>
</div>
++++

[.text-center]
Figure 6.10. DRAKON-diagram heap sorting algorithm a) heapSort(arr); b) heapify(arr,size,i)


The evaluation of the complexity of the heap sorting algorithm presented
in the table

=== 6.9. Sorting comparison

Choosing a sorting algorithm is determined by the following factors: 1

• Time complexity;

• Space complexity;

• Stability/instability.

Knowing the strengths and weaknesses of each of the algorithms
considered allows you to make a choice in favor of a particular sort.
Each algorithm is unique and works best under certain conditions.

[width="100%",cols="26%,25%,24%,25%",options="header", width = 60%]
|===
|Аlgorithm |Worst |Average |Best
|Bubble |O(n2) |O(n2) |O(n)
|Selection |O(n2) |O(n2) |O(n2)
|Insertion |O(n2) |O(n2) |O(n)
|Quick Sort |O(n2) |O(nlog(n)) |O(nlog(n))
|Merge Sort |O(nlog(n)) |O(nlog(n)) |O(nlog(n))
|Shell Sort |O(n(log(n)2) |O(n(log(n)2) |O(n)
|Heap Sort |O(n.log(n)) |O(n.log(n)) |O(n.log(n))
|===

[cols="1,1,1",options="header", width = 60%]
|===
|Аlgorithm |Space comlexity |Stable
|Bubble |O(1) |Stable
|Selection |O(1) |Stable
|Insertion |O(1) |Stable
|Quick Sort |O(1) |Stable
|Merge Sort |O(1) |Stable
|Shell Sort |O(1) |Stable
|Heap Sort |O(1) |Stable
|===

Some common sorting algorithms are inherently stable, such as MergeSort, InsertSort, and BubbleSort. Others, such as QuickSort, HeapSort, and SelectSort, are unstable. For example, we can use the extra space to maintain stability in QuickSort.
